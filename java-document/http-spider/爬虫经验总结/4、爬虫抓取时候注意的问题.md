1、抓取网页步骤

（1）先直接请求，什么都不加，看看能不能拿到数据

curl 或者 postman 看看能不能拿到数据

（2）是否需要代理：数据了大，怕被封

（3）header，cookie，post请求等

添加header防止被封：User-Agent、Referer等做了限制

post请求注意from data 与是否要加content-type

是否需要用cookie，登录后能用多久------> 单独做登录上传cookie管理

（4）请求是否有中间页跳转，302情况

中间页有js，是否需要执行这段js才能跳转

（5）token，加密情况

web：js断点调试加密

app：破解app，寻找加密方式

（6）数据保存

去重：布隆过滤器（hash要分布均匀，尽可能少的hash碰撞；重要数据不能用）；Map（注意多线程安全）；

队列：queue（加锁，是否影响速度）；

保存失败：批量保存失败进行单条保存；多线程还是单线程（唯一索引问题）



2、抓取工具

（1）常用是使用fiddle，

（2）或者直接f12看

（3）关于app抓取，安卓的需要安卓7.0以下才行，苹果需要使用用chartist

有些链接需要苹果手机+苹果电脑才能抓取（证书）

（4）破解app

反解代码：jadx-gui-1.2.0-no-jre-win.exe（需要花时间，部分native代码无法获取）




爬虫：爬取数据获取接口、页面两种情况，也分json，xml，html三种形式的数据，其他的excel，cvs数据等

对不同数据进行不同的处理，若返回json，需对html等进行过滤，排除错误

性能方面：转json是否消耗cpu内存（待测）、正则好还是转json获取好，正则对cpu内存的影响

拆分：做好请求与解析的拆分，数据统一过滤

