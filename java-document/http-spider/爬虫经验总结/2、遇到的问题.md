#### 一、微信报表抓取

https://mp.weixin.qq.com/?lang=zh_CN&token=

背景：需要进行二维码扫码登录，但是用webdriver启动谷歌浏览器后，扫码后确认失败，但正常打开的可以

解决：

1、看是否加一些启动参数去解决，尽可能的与人工打开的浏览器一致（未找到）

options.setExperimentalOption("excludeSwitches", new Object[]{"enable-automation"});加入后依然不可用

```js
window.navigator.webdriver
webdriver打开是true，非是undefined
```

2、去控制一个已打开的浏览器，（不可，每次都会关掉浏览器）

3、换个火狐浏览器去做（可行）

思考：

可能是因为webdriver打开谷歌浏览器有某些浏览器参数被传到服务器，导致被认为是爬虫，但是火狐没有



ps：

谷歌下载：**chromedriver.exe**

火狐下载：**geckodriver.exe**

IE下载：**IEDriverServer.exe**



#### 二、安卓app抓取

Google 做的新安全策略

- 在 Android7.0 及以上的系统中，每个应用可以定义自己的可信 CA 集。
- 默认情况下，应用只会信任系统预装的 CA 证书，而不会信任用户安装的 CA 证书。通过某些抓包软件安装的证书恰恰正属于用户安装的 CA 证书，因此会被视作不安全的证书。

如果想要在 Android7.0 以上抓第三方 APP 的包只能：

1. 逆向加(改)配置文件
2. Root 系统将抓包软件的证书加入系统证书或者 Xposed(也是只有 Root 后才能操作)

当然，如果是自己的 APP 那么在测试阶段加一个配置文件信任所有的证书即可





三、其他

（1）TLSv1.2协议

com.fanli.tools.http.exception.HttpException: javax.net.ssl.SSLException: Received fatal alert: protocol_version

  1：把JDK升级到1.8。由于该网站使用的是TLSv1.2协议，JDK1.8默认是该协议，故把客户端JDK升级到1.8可以解决该问题。

  2：在JDK1.7客户端代码中指定使用的协议System.setProperty("https.protocols", "TLSv1.2")或
  System.setProperty("jdk.tls.client.protocols", "TLSv1.2")。注意，该方法有时候会莫名的失效，原因未知。

  3：在VM参数中设置-Dhttps.protocols=TLSv1.2或-Djdk.tls.client.protocols=TLSv1.2。注意，该方法有时候会莫名的失效，原因未知。

  4：使用第三方库了，参考http://ligaosong.iteye.com/blog/2356346

  5：使用httpclient，其支持配置使用指定协议

  （2）抓取跳转链接:

  ##通过获取finalUrl，或者从跳转返回的内容里获取url

```java
  RegexService service = new DefaultRegexService();
  ScriptEngineManager manager = new ScriptEngineManager();
  //ScriptEngine用来处理脚本解释和求值
  ScriptEngine engine = manager.getEngineByName("js");
  //Bindings:KV对映射，用来在Java应用程序和javascript脚本之间交换信息，即上下文
  Bindings bindings = engine.createBindings();
  engine.eval(service.getSingleGroup(content, "(eval\\([\\s\\S]*\\))").replace("eval", "a="), bindings);
  Object obj = bindings.get("a");
  finalUrl = service.getSingleGroup(obj.toString(), "smzdmhref\\s*=\\s*'([^']*+)'");
  if(StringUtils.isNotEmpty(finalUrl)){
            webLink = spicalDoUrl(finalUrl,content);
  }
```

  

（3）robots协议

  很多网站对爬虫有所限制，可以先看一下该网站的robots协议，就是某个网站/robots.tex。例如www.baidu.com/robots.txt，从而查看哪些是可以用爬虫访问的。

  User-agent: 这里是爬虫的名字

  Disallow: /该爬虫不允许访问的内容



  （4）关于签名破解：

  1）找到相应的js：通过访问的url，在js中找到相应的post，get的url地址

  2）然后通过这个地址找到相应的token或者sign参数

  3）通过参数，查看前后文，去找生成该参数的方法

  4）将相应的方法转换成java方法，或者通过java调用js方式去实现